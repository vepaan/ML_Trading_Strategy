{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  503 of 503 completed\n"
     ]
    }
   ],
   "source": [
    "#Downloading and formatting the dataset\n",
    "\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import pandas_ta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "sp500['Symbol'] = sp500['Symbol'].str.replace('.', '-')\n",
    "symbols_list = sp500['Symbol'].unique().tolist()\n",
    "\n",
    "end_date = dt.datetime.now()\n",
    "start_date = pd.to_datetime(end_date) - pd.DateOffset(365*8)\n",
    "\n",
    "df = yf.download(tickers = symbols_list, start = start_date, end = end_date)\n",
    "df = df.stack()\n",
    "df.index.names = ['date', 'ticker']\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the technical indicators\n",
    "\n",
    "df['garman_klass_vol'] = ((np.log(df['high']) - np.log(df['low'])) ** 2) / 2 - (2 * np.log(2) - 1) * ((np.log(df['adj close']) - np.log(df['open'])) ** 2)\n",
    "df['rsi'] = df.groupby(level = 1)['adj close'].transform(lambda x: pandas_ta.rsi(close = x, length = 20))\n",
    "\n",
    "df['bb_low'] = df.groupby(level = 1)['adj close'].transform(lambda x: pandas_ta.bbands(close = np.log1p(x), length = 20).iloc[:,0])\n",
    "df['bb_mid'] = df.groupby(level = 1)['adj close'].transform(lambda x: pandas_ta.bbands(close = np.log1p(x), length = 20).iloc[:,1]) \n",
    "df['bb_high'] = df.groupby(level = 1)['adj close'].transform(lambda x: pandas_ta.bbands(close = np.log1p(x), length = 20).iloc[:,2])\n",
    "\n",
    "def compute_atr(stock_data):\n",
    "    atr = pandas_ta.atr(high = stock_data['high'],\n",
    "                        low = stock_data['low'],\n",
    "                        close = stock_data['close'],\n",
    "                        length = 14)\n",
    "    return atr.sub(atr.mean()).div(atr.std())\n",
    "\n",
    "df['atr'] = df.groupby(level = 1, group_keys = False).apply(compute_atr)\n",
    "\n",
    "def compute_macd(close):\n",
    "    macd = pandas_ta.macd(close=close, length=20)\n",
    "    if macd is not None:\n",
    "        return macd.iloc[:, 0].sub(macd.iloc[:, 0].mean()).div(macd.iloc[:, 0].std())\n",
    "    else:\n",
    "        return pd.Series([np.nan] * len(close), index=close.index)\n",
    "\n",
    "df['macd'] = df.groupby(level = 1, group_keys = False)['adj close'].apply(compute_macd)\n",
    "\n",
    "df['dollar_volume'] = (df['adj close'] * df['volume'])/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate to monthly level and filter top 150 most liquid stocks for each month\n",
    "'''This is done to reduce training time for the ML model and experiment with strats'''\n",
    "#This computes the average monthly dollar volume to give a sense of liquidity\n",
    "\n",
    "last_cols = [c for c in df.columns.unique(0) if c not in ['dollar_volume', 'volume',\n",
    "                                                          'open','high','low','close']]\n",
    "\n",
    "data = pd.concat([df.unstack('ticker')['dollar_volume'].resample('M').mean().stack().to_frame('dollar_volume'),\n",
    "          df.unstack()[last_cols].resample('M').last().stack('ticker')], axis = 1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the 5 year rolling average rolling average for each stock\n",
    "#and use this value to filter out top 150 most liquid stocks for each month\n",
    "\n",
    "data['dollar_volume'] = (data.loc[:, 'dollar_volume'].unstack('ticker').rolling(5*12, min_periods=12).mean().stack())\n",
    "\n",
    "data['dollar_vol_rank'] = data.groupby('date')['dollar_volume'].rank(ascending = False)\n",
    "\n",
    "#We are finding the 150 most liquid stocks then dropping the columns that we dont need\n",
    "data = data[data['dollar_vol_rank']<150].drop(['dollar_volume', 'dollar_vol_rank'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Removing the timezones from the dataframe data as they are not necessary'''\n",
    "#Remove the time zone from the 'date' index of data\n",
    "data.index = data.index.set_levels([data.index.levels[0].tz_localize(None), data.index.levels[1]])\n",
    "\n",
    "#Convert the 'date' index to only the date part (remove time part)\n",
    "data.index = data.index.set_levels([data.index.levels[0].date, data.index.levels[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating monthly returns for different time horizons as features\n",
    "'''To capture time series dynamics that reflect, for instance, momentum patterns, we\n",
    "compute historical returns using the method .pct_change(lag), that is, returns over\n",
    "various monthly periods as identified by lags'''\n",
    "\n",
    "def calculate_returns(df):\n",
    "\n",
    "    outlier_cutoff = 0.005 #only the 99.005 percentile is considered and values above this percentile are assigned the same value as the cutoff\n",
    "    lags = [1, 2, 3, 6, 9, 12]\n",
    "\n",
    "    for lag in lags:\n",
    "        df[f'return_{lag}m'] = (df['adj close']\n",
    "                            .pct_change(lag)\n",
    "                            .pipe(lambda x: x.clip(lower = x.quantile(outlier_cutoff),\n",
    "                                                    upper = x.quantile(1 - outlier_cutoff)))\n",
    "                                .add(1)\n",
    "                                .pow(1/lag)\n",
    "                                .sub(1))\n",
    "    return df\n",
    "    \n",
    "data = data.groupby(level = 1, group_keys = False).apply(calculate_returns).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Download FAMA-FRENCH factors and calculate rolling factor betas'''\n",
    "#we are introducing the FAMA french data to estimate the exposure of assets to\n",
    "#common risk factors with linear regression.\n",
    "#five factors are market risk size, value, operating probability and investment\n",
    "#these have been shown to empirically assess asset returns\n",
    "#we can access historical returns using pandas-datareader and estimate historial \n",
    "#exposures using Rollingols model\n",
    "\n",
    "factor_data = web.DataReader('F-F_Research_Data_5_Factors_2x3',\n",
    "               'famafrench',\n",
    "               start = '2010')[0].drop('RF', axis = 1)\n",
    "\n",
    "factor_data.index = factor_data.index.to_timestamp() #but this returns beginning of month data, which we have to fix to end of month\n",
    "\n",
    "factor_data = factor_data.resample('M').last().div(100)\n",
    "\n",
    "factor_data.index.name = 'date'\n",
    "#now we just combine this with the 1 month return from previous code\n",
    "\n",
    "# '''We are adding timezones to factordata date as data has timezones'''\n",
    "# #Check if 'factor_data' is timezone-aware\n",
    "# if factor_data.index.tz is None:\n",
    "#     # If it's not timezone-aware, localize it to UTC (or any timezone you prefer)\n",
    "#     factor_data.index = factor_data.index.tz_localize('UTC')\n",
    "# else:\n",
    "#     # If it's already timezone-aware, ensure it's in the same timezone (UTC)\n",
    "#     factor_data.index = factor_data.index.tz_convert('UTC')\n",
    "# '''end of bug fix'''\n",
    "\n",
    "factor_data = factor_data.join(data['return_1m']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Filter out stocks with less than 10 months of data'''\n",
    "#we do this because we are going to use rolling window for the regression of 24mnths\n",
    "#stocks without enough data can break our functions\n",
    "\n",
    "observations = factor_data.groupby(level = 1).size()\n",
    "valid_stocks = observations[observations >= 10] #more than 10 months data\n",
    "\n",
    "factor_data = factor_data[factor_data.index.get_level_values('ticker').isin(valid_stocks.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculating the rolling factor betas'''\n",
    "\n",
    "betas = (factor_data.groupby(level = 1,\n",
    "                    group_keys = False)\n",
    "        .apply(lambda x: RollingOLS(endog = x['return_1m'],\n",
    "                                    exog = sm.add_constant(x.drop('return_1m', axis = 1)),\n",
    "                                    window = min(24, x.shape[0]), #if less than 24 months data, still calculate ols\n",
    "                                    min_nobs = len(x.columns)+1)\n",
    "        .fit(params_only = True)\n",
    "        .params\n",
    "        .drop('const', axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Joining the betas to the features as well'''\n",
    "#however we cannot blindly join betas to our dataset\n",
    "#this is because we use the factor at the beginning of the month and return at the end of the month to compute betas.\n",
    "#so at the end of the month, well be able to run the regression and have the betas but we will have them in the next month.\n",
    "#so the betas of oct 31 will be known in nov 1\n",
    "#so we shift the betas one month ahead before joining\n",
    "#doing betas.shift() will simply shift betas forward by ticker, so XOM will get WMT's beta\n",
    "\n",
    "data = data.join( betas.groupby('ticker').shift() )\n",
    "\n",
    "#removing the NaN values in the FAMA-French columns\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "data.loc[:, factors] = data.groupby('ticker', group_keys = False)[factors].apply(lambda x: x.fillna(x.mean()))\n",
    "data = data.dropna()\n",
    "data = data.drop('adj close', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We'll use a K-means clustering alg to group similar assets based on their features\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Now we will start with the ML model as the data is complete'''\n",
    "#We have the top 150 most liquid stocks at the end of each month.\n",
    "#Our job is to build portfolios at the EOM and evaluate which stocks to put in.\n",
    "\n",
    "'''We'll use a K-means clustering alg to group similar assets based on their features'''\n",
    "#Split the stocks into four diff groups (optimal number of clusters as said by a quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Now we'll assign a cluster to each stock\n",
    "def get_clusters(df):\n",
    "    df['cluster'] = KMeans(n_clusters = 4, #4 clusters: 0,1,2,3\n",
    "                           random_state = 0,  #clustering works with random seed\n",
    "                           init = 'random').fit(df).labels_\n",
    "    return df\n",
    "\n",
    "data = data.dropna().groupby('date', group_keys = False).apply(get_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Visualizing the clustering'''\n",
    "#Since we didnt normalize the RSI before, we can use this for visualization\n",
    "#We cannot visualize all the features in even a 3 dimensional space\n",
    "\n",
    "def plot_clusters(data):\n",
    "\n",
    "    cluster_0 = data[data['cluster'] == 0]\n",
    "    cluster_1 = data[data['cluster'] == 1]\n",
    "    cluster_2 = data[data['cluster'] == 2]\n",
    "    cluster_3 = data[data['cluster'] == 3]\n",
    "\n",
    "    #5th index (6th column) is the ATR in the dataframe\n",
    "    #1st index (2nd column) is the RSI in the dataframe\n",
    "    #Plotting ATR against RSI\n",
    "    plt.scatter(cluster_0.iloc[:, 5], cluster_0.iloc[:, 1], color = 'red', label = 'cluster_0')\n",
    "    plt.scatter(cluster_1.iloc[:, 5], cluster_1.iloc[:, 1], color = 'green', label = 'cluster_1')\n",
    "    plt.scatter(cluster_2.iloc[:, 5], cluster_2.iloc[:, 1], color = 'blue', label = 'cluster_2')\n",
    "    plt.scatter(cluster_3.iloc[:, 5], cluster_3.iloc[:, 1], color = 'black', label = 'cluster_3')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually plotting\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "for i in data.index.get_level_values('date').unique().tolist():\n",
    "    g = data.xs(i, level = 0)\n",
    "    plt.title(f'Date {i}')\n",
    "    plot_clusters(g)\n",
    "\n",
    "#an observation i made was that the clusters are grouped based on RSI values\n",
    "#clusters more or less are grouped as >60, 50-60, 40-50 and <40\n",
    "#however since K-means centroids are random, the clusters are also a bit random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rsi_values = [30, 45, 55, 70]\n",
    "\n",
    "initial_centroids = np.zeros((len(target_rsi_values), 18)) #checked the k means docs for this. 18 represents the number of columns of data\n",
    "\n",
    "initial_centroids[:, 1] = target_rsi_values #rsi value column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now we are going to apply the newfound knowledge about initial centroids to the K-means clustering alg'''\n",
    "#We will drop the original clusters\n",
    "\n",
    "data = data.drop('cluster', axis = 1)\n",
    "\n",
    "def get_clusters(df):\n",
    "    df['cluster'] = KMeans(n_clusters = 4, #4 clusters: 0,1,2,3\n",
    "                           random_state = 0,  #clustering works with initial centroids\n",
    "                           init = initial_centroids).fit(df).labels_\n",
    "    return df\n",
    "\n",
    "data = data.dropna().groupby('date', group_keys = False).apply(get_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "for i in data.index.get_level_values('date').unique().tolist():\n",
    "    g = data.xs(i, level = 0)\n",
    "    plt.title(f'Date {i}')\n",
    "    plot_clusters(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''At the beginnng of each new month, select stocks based on cluster and form'''\n",
    "'''a portfolio based on efficient frointier max sharpe ratio optimization'''\n",
    "#momentum is persistent, and our idea would be that stocks clustered around 70 rsi centroid should\n",
    "#continue to outperform in the following month, So we would select the stocks of cluster 3\n",
    "\n",
    "filtered_df = data[data['cluster'] == 3].copy()\n",
    "\n",
    "filtered_df = filtered_df.reset_index(level = 1)\n",
    "filtered_df.index = filtered_df.index + pd.DateOffset(1) #moving each day with one so we go from end of month to beginning of next month\n",
    "filtered_df = filtered_df.reset_index().set_index(['date', 'ticker'])\n",
    "\n",
    "dates = filtered_df.index.get_level_values('date').unique().tolist()\n",
    "\n",
    "fixed_dates = {}\n",
    "\n",
    "for d in dates:\n",
    "    \n",
    "    fixed_dates[d.strftime('%Y-%m-%d')] = filtered_df.xs(d, level = 0).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEFINE PORTFOLIO OPTIMIZATION FUNCTION'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''DEFINE PORTFOLIO OPTIMIZATION FUNCTION'''\n",
    "#we will use the PyPortfolioOpt package and EfficientFrontier to maximize the sharpe ratio\n",
    "#to optimize the weights of a given portfolio, we would need to supply last one year prices to the function\n",
    "#apply single stock weight bounds constraint for diversification (minimum half of equally weight and maximum 10% of portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "\n",
    "#the lower bound is here because if i just put in 0, the porfolio might be designed such that:\n",
    "#1 stock has a weight of 1 and the others have a weight of 0\n",
    "\n",
    "def optimize_weights(prices, lower_bound=0):\n",
    "\n",
    "    returns = expected_returns.mean_historical_return(prices = prices,\n",
    "                                                      frequency = 252) #1 yr of trading data\n",
    "    cov = risk_models.sample_cov(prices = prices,\n",
    "                                 frequency = 252)\n",
    "    ef = EfficientFrontier(expected_returns = returns,\n",
    "                           cov_matrix = cov,\n",
    "                           weight_bounds = (lower_bound, .1),\n",
    "                           solver = 'SCS')\n",
    "    weights = ef.max_sharpe()\n",
    "\n",
    "    return ef.clean_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  154 of 154 completed\n"
     ]
    }
   ],
   "source": [
    "stocks = data.index.get_level_values('ticker').unique().tolist()\n",
    "\n",
    "new_df = yf.download(tickers = stocks,\n",
    "                     start = data.index.get_level_values('date').unique()[0]-pd.DateOffset(months=12),\n",
    "                     end = data.index.get_level_values('date').unique()[-1]) #past year of data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate daily returns for each stock whcih could land up in the portfolio\n",
    "#Loop over each month start, select stocks for the month and calculate weights for the nextmonth\n",
    "#if max sharpe ratio optimization fails for a monnth, apply equally weighted weights\n",
    "#calculate each day portfolio return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "retruns_dataframe = np.log(new_df['Adj Close']).diff()\n",
    "portfolio_df = pd.DataFrame()\n",
    "\n",
    "for start_date in fixed_dates.keys():\n",
    "\n",
    "    end_date = (pd.to_datetime(start_date) + pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    cols = fixed_dates[start_date] #stocks for each month which will form the portfolio\n",
    "\n",
    "    optimization_start_date = (pd.to_datetime(start_date) - pd.DateOffset(months=12)).strftime('%Y-%m-%d')\n",
    "    optimization_end_date = (pd.to_datetime(start_date) - pd.DateOffset(days=1)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now creating an optimization dataframe to calculate the weights of the stocks in the portfolio\n",
    "\n",
    "optimization_df = new_df['Adj Close'][optimization_start_date:optimization_end_date][fixed_dates[dates[-1].strftime('%Y-%m-%d')]] #latest date (1 day more than optimization end date) that is in the dict fixed_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = optimize_weights(prices = optimization_df,\n",
    "                           lower_bound = round(1/(len(optimization_df.columns*2)),3)) #half of the weight of an equally weighted portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVDA 0.03401\n"
     ]
    }
   ],
   "source": [
    "for wt in weights:\n",
    "    if weights[wt] != 0.021:\n",
    "        print(wt, weights[wt])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
